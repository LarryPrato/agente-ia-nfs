# Variáveis de Ambiente para o Projeto AI Agents de Notas Fiscais

# Ambiente de Execução: 'cloud' para usar LLM via API, 'local' para usar LLM GGUF localmente
ENV=local # Ou 'cloud'

# --- Configurações para LLM em Nuvem (se ENV=cloud) ---
# Se você estiver usando um serviço de LLM que requer API Key (ex: OpenAI, Cohere, alguns endpoints HF)
# Descomente e substitua pela sua chave. NUNCA envie este arquivo para o Git!
# LLM_CLOUD_API_KEY="sk-YOUR_CLOUD_LLM_API_KEY_HERE"

# Nome do modelo LLM a ser usado no modo 'cloud'. Pode ser um modelo HuggingFace publicamente acessível.
# Ex: google/flan-t5-base, mistralai/Mistral-7B-Instruct-v0.2
LLM_CLOUD_MODEL_NAME="google/flan-t5-base"

# --- Configurações para LLM Local (se ENV=local) ---
# Nome do arquivo GGUF para o modelo local. Este arquivo deve estar na pasta 'models/'.
# Use 'make download-model' para baixar um exemplo (Mistral-7B).
LLM_LOCAL_MODEL_NAME="mistral-7b-instruct-v0.2.Q4_K_M.gguf"

# --- Configurações da API FastAPI ---
API_HOST="0.0.0.0" # O host da API (dentro do Docker)
API_PORT="8000"    # A porta da API (dentro do Docker)

# --- Configurações do Streamlit (se executado separadamente ou como serviço) ---
STREAMLIT_PORT="8501" # A porta do Streamlit (dentro do Docker)